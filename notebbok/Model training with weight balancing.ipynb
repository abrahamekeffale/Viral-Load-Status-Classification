{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(r'C:\\\\Users\\\\HP\\\\Desktop\\\\updated_research_nutrition_data.xlsx', engine='openpyxl')\n",
    "\n",
    "# Define the feature columns and the target variable\n",
    "X = df.drop(columns=['Viral_Load_Status'])\n",
    "y = df['Viral_Load_Status']\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the models to be trained\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"LightGBM\": lgb.LGBMClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "log_losses = []\n",
    "model_names = []\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "    else:\n",
    "        y_prob = model.decision_function(X_test)\n",
    "    \n",
    "    # Store evaluation metrics and log loss score\n",
    "    results[model_name] = {\n",
    "        \"Classification Report\": classification_report(y_test, y_pred),\n",
    "        \"Log Loss\": log_loss(y_test, y_prob)\n",
    "    }\n",
    "    \n",
    "    log_losses.append(results[model_name][\"Log Loss\"])\n",
    "    model_names.append(model_name)\n",
    "\n",
    "# Plot Log Loss scores for all models with scores in the bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(model_names, log_losses, color='skyblue')\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Log Loss Scores for Different Models')\n",
    "\n",
    "# Add scores to the bars\n",
    "for bar, score in zip(bars, log_losses):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
    "             f'{score:.2f}', va='center', ha='left', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print classification reports for all models\n",
    "for model_name, result in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(result[\"Classification Report\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
